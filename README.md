#OmniSearch CLI: Architectural Blueprint for an Advanced, Interactive Google OSINT Framework1. Executive SummaryThe domain of Open Source Intelligence (OSINT) is undergoing a paradigm shift, moving from static, manual query execution to dynamic, automated intelligence gathering. At the forefront of this evolution is the technique known as "Google Dorking"—the utilization of advanced search operators to uncover sensitive information indexed by search engines. While repositories like the Google Hacking Database (GHDB) provide a foundational lexicon of vulnerability signatures, the tooling available to practitioners remains fragmented, often relying on disparate scripts that lack cohesion, state management, or advanced interactive capabilities.This report outlines the comprehensive architectural design for OmniSearch CLI, a proposed Linux-based command-line interface tool designed to become the preeminent utility for Google search-based OSINT. OmniSearch CLI is conceptualized not merely as a scanner, but as an integrated intelligence platform that harmonizes vast datasets of known vulnerabilities with proprietary, user-defined targeting logic.The design philosophy prioritizes three core tenets: Aggregated Intelligence, merging public GHDB entries with high-value proprietary dorks; Interactive Dynamism, leveraging modern Terminal User Interface (TUI) frameworks to provide a responsive, dashboard-like experience; and Heuristic Precision, employing advanced validation logic to minimize false positives and maximize the relevance of intelligence gathered. This document details the technical specifications, database schema, logic engines, and user interface paradigms required to realize this vision, drawing heavily upon the provided proprietary datasets (sopro.txt and eval.txt) to inform its unique functional requirements.2. Intelligence Asset Decomposition and TaxonomyThe efficacy of any intelligence tool is directly proportional to the quality and structure of its underlying data. OmniSearch CLI is architected to ingest, normalize, and operationalize three distinct streams of intelligence: the public Google Hacking Database (GHDB), the proprietary "Sopro" dataset, and the generative "Eval" logic dataset. Understanding the nuanced structure of these assets is a prerequisite for designing the database schema and query logic.2.1 The Google Hacking Database (GHDB) BaselineThe GHDB serves as the foundational knowledge base for the system. It is a mature, categorized repository of search queries that have been vetted by the security community to identify generic vulnerabilities, exposed files, and server misconfigurations.1The architecture treats GHDB entries as "Baseline Signatures." These are static queries that map to well-defined categories such as "Footholds," "Files Containing Usernames," and "Vulnerable Servers".2 The ingestion pipeline for GHDB data requires a scraper or API connector to the Exploit-DB repository, parsing fields such as the dork query, the author, the date added, and the textual description. This data provides the broad coverage necessary for initial reconnaissance but lacks the specificity required for targeted high-value investigations.32.2 The "Sopro" High-Value Target DatasetAnalysis of the provided sopro.txt file reveals a dataset that differs fundamentally from the GHDB. While the GHDB is generic, the Sopro dataset is a curated collection of High-Value Targets (HVTs) focused on modern SaaS infrastructure, DevOps workflows, and financial data leaks.4Detailed examination of the content within sopro.txt indicates several distinct clusters of intelligence that define the tool's specialized capabilities:SaaS Ecosystem Reconnaissance: A significant portion of the dataset targets specific third-party platforms rather than self-hosted servers. Dorks such as site:trello.com "confidential" | "deck" | "roadmap" -public and site:notion.so intext:"seed phrase" demonstrate a focus on externalized corporate knowledge bases. This necessitates an architectural component capable of distinguishing between "Infrastructure Dorks" (servers) and "Platform Dorks" (SaaS).FinOps and Business Intelligence: The presence of queries like inurl:/wp-content/uploads/ "invoice" filetype:pdf and intitle:"billing portal" intext:"recurly" indicates a specialized logic for uncovering financial exposure. The system must categorize these as "Financial Intelligence" to prioritize them during audits.DevOps and Cloud Secrets: The dataset aggressively targets configuration files and API keys, utilizing patterns like ext:env, AWS_ACCESS_KEY_ID, FIREBASE_CONFIG, and docker. This suggests the need for a regex-based validation engine to verify if the discovered files actually contain keys rather than just templates.4The "Sopro" dataset functions as the "Special Forces" component of the database, deployed when the user requires high-fidelity, high-impact results rather than broad surface area mapping.2.3 The "Eval" Generative Logic EngineThe eval.txt dataset introduces a unique architectural requirement: it is not a list of static dorks, but a Grammar for generating them.4 It provides a structural blueprint for constructing queries based on specific parameters, described as <ptype> (Page Type), <pformat> (Page Format), and <keyword>.The analysis of eval.txt reveals a logic structure that must be codified into the tool's core engine:Parameter-Based Permutation: The text explicitly lists URL parameters associated with different industries. For example, the "Gaming" category uses parameters like GameID=, server_id=, and gpu=, while the "Shopping" category uses cartId=, shopcd=, and transaction_id=.Format Abstraction: The logic separates the file extension (.php, .asp, .html) from the parameter, allowing for the combinatorial generation of dorks. A single intent (e.g., "Find Gaming Servers") can be algorithmically expanded into dozens of specific queries: inurl:.php?GameID=, inurl:.asp?GameID=, inurl:.php?server_id=.Heuristic Logic: The document distinguishes between "Trash Dorks," "Low Quality (LQ) Dorks," and "High Quality (HQ) Dorks," implying a hierarchy of effectiveness. The system must implement a ranking algorithm that prioritizes "HQ" structures (e.g., those using specific ID parameters) over "Trash" structures (e.g., generic file extensions).4This dataset dictates the creation of a "Wizard" interface, where users select a target industry and the system procedurally generates the optimal search queries.3. System Architecture and Technology StackTo meet the requirement of being the "world's best," OmniSearch CLI must be built on a foundation that ensures performance, extensibility, and user experience. The architecture is monolithic in distribution (a single installable tool) but modular in design, separating the User Interface, the Logic Engine, the Database, and the Network Layer.3.1 Core Language and Framework SelectionPython 3.10+ is the designated language for this architecture. While languages like Go offer raw concurrency performance 5, Python's dominance in the OSINT and scraping ecosystem—exemplified by libraries like requests, BeautifulSoup, and Playwright—makes it the pragmatic choice for a tool that relies heavily on parsing unstructured web data.6Terminal User Interface (TUI) Framework: The project will utilize Textual. Unlike older libraries such as curses or urwid, Textual offers a modern, CSS-driven layout engine, reactive state management, and built-in widget support (DataTables, Trees, Modals). This is critical for meeting the "user-friendly" and "interactive" requirements, as it allows for the creation of complex dashboards that function similarly to web applications but reside entirely within the terminal.73.2 Database EngineSQLite with the FTS5 (Full-Text Search) extension is the chosen storage engine.Justification: The tool must manage thousands of dorks from the GHDB and custom files. Standard string matching is inefficient at this scale. FTS5 allows for instantaneous, Google-like searching within the local dork database (e.g., searching for "password" finds relevant dorks in milliseconds).Portability: SQLite requires no server process, ensuring the tool remains a self-contained CLI utility that can be easily distributed or run in air-gapped environments.93.3 Network and Evasion LayerTo survive the aggressive anti-bot measures of modern search engines, the network layer must be sophisticated.Browser Automation: Playwright (Python) is selected over Selenium. Playwright offers superior stealth capabilities, faster execution via its async API, and better handling of modern web features like Shadow DOM, which is essential for parsing complex search result pages.10HTTP Client: For non-browser tasks (API calls, downloading file content), httpx is chosen for its native async support, allowing the UI to remain responsive while network operations occur in the background.114. Database Schema DesignThe database schema is the structural blueprint that normalizes the chaotic inputs from sopro.txt and eval.txt into a coherent query engine.4.1 Normalized Dork StorageThe core logic relies on a central dorks table that unifies all data sources.Column NameData TypeDescriptionidINTEGER PKUnique Identifier.queryTEXTThe actual search string (e.g., filetype:env intext:DB_PASSWORD).descriptionTEXTContextual explanation of the target.category_idINTEGER FKLink to the categorization table.sourceTEXTOrigin tag: 'GHDB', 'SOPRO', 'EVAL_GEN', 'USER'.risk_scoreINTEGER1-10 Scale. 'Sopro' dorks containing 'password' get 10.tagsTEXT (JSON)Searchable tags (e.g., ['aws', 'credentials', 'high-value']).last_validatedDATETIMETimestamp of the last successful verification.4.2 Template Engine Schema (Eval.txt)To support the generative logic of eval.txt, the database must store components rather than full strings.Table: templatesDescriptionidPKstructuree.g., site:{domain} inurl:{pformat}?{ptype}=contexte.g., 'Gaming', 'Shopping', 'General'Table: template_varsDescriptionidPKvar_type'ptype', 'pformat', 'keyword'valuee.g., item_id, cartId, .php, .aspcategory_tage.g., 'Gaming', 'Shopping'This schema allows the "Wizard" to query SELECT value FROM template_vars WHERE var_type='ptype' AND category_tag='Gaming' to populate the generation options dynamically.44.3 Full-Text Search ConfigurationA virtual table dorks_fts is created using SQLite's FTS5 module to index the query and description columns. This enables the user to type "camera" in the TUI and instantly filter thousands of dorks to find those related to webcam exposures, regardless of whether they came from the GHDB or a custom file.95. The User Interface: A "Hacker Dashboard"The User Interface is the critical differentiator for OmniSearch CLI. Unlike traditional tools that output a linear stream of text, OmniSearch employs a persistent, event-driven TUI that organizes information spatially.5.1 Dashboard Layout ArchitectureThe interface is constructed using a Dock Layout, dividing the terminal screen into three functional zones:The Navigation Sidebar (Left):Widget: Tree view.Function: Displays the hierarchy of Dork Categories.Root Node: "Databases"Child: "GHDB" (Expanded to show categories like "Footholds", "Vulnerable Files").Child: "Sopro Targets" (Expanded to show "SaaS", "FinOps", "DevOps").4Child: "My Scans" (History).Interaction: Clicking or navigating to a node filters the central view. This leverages the hierarchical nature of the data found in the research.12The Command Center (Center):Top Bar: An Input widget for the target domain (e.g., target-company.com).Data Grid: A DataTable widget displaying the loaded dorks.Columns: ID, Query, Source, Risk, Estimated Results.Styling: Rows derived from sopro.txt are highlighted in bold red to indicate their high-value nature. Rows from the "Wizard" are highlighted in blue.Result View: A secondary DataTable that populates with search results (URLs) as they are found. This table supports sorting by HTTP status code and response size.13The Status Footer (Bottom):Widget: Static / Sparkline.Function: Displays operational metrics: "Requests/Min", "Proxy Health", "Active Threads".Visuals: A live sparkline graph showing the rate of incoming results, providing immediate visual feedback on the search's effectiveness.85.2 The "Dork Wizard" ModalThe "Dork Wizard" is a specialized screen triggered by a hotkey (e.g., F2). It is the direct implementation of the eval.txt logic.Workflow:Intent Selection: A Select widget asks the user for the campaign type: "Gaming", "Shopping", "Streaming", or "General".Target Input: User enters the keyword (e.g., "Fortnite" or "Shopify").Parameter Tuning: The system presents a list of available <ptypes> (derived from the template_vars table) using a SelectionList. The user can check specific parameters (e.g., cartId, transaction_id) or "Select All".Generation: The system permutes the inputs into a list of dorks and presents them for review.Execution: The user confirms, and the dorks are injected into the main execution queue.This interactive flow transforms the abstract logic of eval.txt into a tangible, usable workflow.146. Logic Engine: Operationalizing the DataThe tool's logic engine is responsible for transforming raw text data into executable search strategies.6.1 Parsing the "Sopro" LogicThe sopro.txt file uses complex boolean logic that must be parsed correctly.Boolean Parsing: Dorks like site:pastebin.com "api_key" | "secret" | "token" contain the pipe | operator, which translates to the Google OR operator. The parser normalizes this to site:pastebin.com ("api_key" OR "secret" OR "token") to ensure compatibility with the search engine's syntax requirements.Exclusion Logic: The minus operator - (e.g., -github) is critical for noise reduction. The parser ensures these exclusions are appended to the end of the query string to maintain their precedence.Category Inference: The engine scans the dork for keywords. If ext:env, DB_PASSWORD, or AWS_ACCESS_KEY is present, the dork is tagged as and. If invoice, budget, or salary is present, it is tagged ``. This automated tagging allows users to filter the sopro dataset effectively without manual sorting.46.2 The "Eval" Generation AlgorithmThe eval.txt logic dictates a specific construction pattern: <sfunct><keyword><pformat><ptype>.Algorithm:Pythondef generate_dorks(target, context, selected_ptypes):
#    dorks =
#    formats = get_formats_from_db() # e.g.,.php,.asp
#    for fmt in formats:
#        for ptype in selected_ptypes:
#            # Logic from eval.txt: constructing the specific query structure
#            # Example: site:target.com inurl:.php?item_id=
#            query = f"site:{target} inurl:{fmt}?{ptype}"
#            dorks.append(query)
#    return dorks
#Query Optimization: To avoid sending hundreds of requests, the generator optimizes by grouping parameters using the OR operator, effectively packing multiple eval.txt patterns into a single Google query (e.g., inurl:.php? (item_id= OR cartId= OR shopcd=)). This respects Google's character limit while maximizing coverage.47. Network Layer and Evasion StrategyThe primary challenge in Google OSINT is avoiding detection. Google employs sophisticated anti-bot systems that analyze user behavior, IP reputation, and TLS fingerprints.7.1 Pluggable Search BackendsTo ensure the tool remains usable in various environments, the search execution is decoupled from the logic via a Strategy Pattern.Backend A: Browser-Based Scraper (Playwright)This is the default mode for "free" usage. It launches a headless Chromium instance.Stealth Techniques: It patches the navigator.webdriver property to hide the automation flag. It randomizes the viewport size and user-agent string for each session.Interaction: It mimics human behavior by introducing random "jitter" (delays) between keystrokes when typing the query and performs random mouse movements before clicking search results. This behavioral mimicry is crucial for bypassing CAPTCHAs.10Backend B: API Connector (SerpApi/Google CSE)For enterprise users requiring 100% stability, the tool supports API keys.SerpApi: Offloads the scraping to a third-party service, effectively outsourcing the evasion problem.16Google Custom Search JSON API: Allows for official, paid access to search results. The tool's configuration manager handles the API key and Custom Search Engine (CSE) ID.177.2 Rate Limiting and Token BucketsTo manage the request flow and prevent "429 Too Many Requests" errors, the Network Manager implements a Token Bucket algorithm.Mechanism: The user configures a rate (e.g., "5 requests per minute"). The bucket fills with 5 tokens every minute. Each search consumes one token. If the bucket is empty, the search worker pauses.Dynamic Backoff: If a 429 status is detected, the system triggers a "Cool Down" state, exponentially increasing the wait time before retrying.7.3 Proxy RotationA ProxyManager class handles IP rotation.Source: It accepts a proxies.txt file or an API endpoint for a proxy provider.Rotation Logic: It rotates the IP address after a configurable number of requests (N) or immediately upon detecting a CAPTCHA or block.Health Checks: A background thread periodically checks the validity of proxies in the pool, removing dead ones to prevent failed searches.188. Advanced Heuristic ValidationA significant weakness of current tools is the prevalence of "Soft 404s" (pages that return 200 OK but contain no data) and false positives (e.g., a .env.example file instead of a real .env file). OmniSearch CLI incorporates a Validator Module to address this.8.1 Content AnalysisWhen a potential result is found (e.g., a URL pointing to a .env file), the Validator performs a secondary HTTP GET request to download the content.Regex Matching: It scans the response body against a library of high-confidence regular expressions.Example: If the dork looked for DB_PASSWORD, the validator checks if the text actually contains DB_PASSWORD= followed by a non-whitespace string.19Confidence Scoring: Results are assigned a "Confidence Score" (High/Medium/Low). A match on the filename is Low; a match on the content is High.8.2 Soft 404 DetectionThe tool calculates the entropy and length of the response. If the response is identical (or highly similar via fuzzy hashing) to the site's root page or a known error page, it is flagged as a Soft 404 and hidden from the main results view to reduce noise.209. Implementation Guide and Configuration9.1 Installation and DependenciesThe tool is packaged as a Python module.Bashpip install omnisearch-cli
#omnisearch setup  # Launches the configuration wizard
#Dependencies include textual (UI), playwright (Scraping), httpx (Network), sqlite-utils (Database), and pluggy (Plugins).9.2 Plugin ArchitectureTo ensure future extensibility, OmniSearch uses pluggy.21 This allows the community to write plugins that:Add new Dork Sources (e.g., a scraper for a new dork repository).Add new Validators (e.g., a specific validator for a new type of crypto wallet).Add new Export Formats (e.g., export to Maltego or CSV).9.3 Usage WorkflowInitialization: omnisearch run opens the TUI.Targeting: User enters the scope (domain).Selection: User selects the "SaaS" category from the Sidebar (sourced from sopro.txt).Execution: The tool runs the checks, rotating proxies in the background.Review: High-confidence results (validated via regex) appear at the top of the DataTable in red.Drill-Down: User clicks a result to see the extracted snippet (e.g., the specific API key found).Export: User presses Ctrl+E to generate a JSON report for their client.10. ConclusionOmniSearch CLI represents a comprehensive evolution in OSINT tooling. By rigorously analyzing and operationalizing the sopro.txt and eval.txt datasets, it transforms static lists into a dynamic, queryable intelligence engine. The integration of a modern TUI, robust database architecture, and advanced evasion strategies addresses the limitations of existing scripts, providing analysts with a powerful, persistent, and user-friendly platform for web reconnaissance. This design ensures that the tool is not only effective at finding information but also resilient, scalable, and adaptable to the ever-changing landscape of the open web.
